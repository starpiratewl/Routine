#############################################################################################################################
## single sample t-test
#############################################################################################################################

## z test when z score = -1, z scores = (sample_mean - population_mean)/standard error
pnorm(-1, mean = 0, sd = 1)
## 0.1586553

## one side hypothesis testing, the result is the same as the result on page 183
pnorm(31, mean = 30, sd = 0.6, lower.tail = F)
##[1] 0.04779035

## two sided hypothesis testing, the result is the same as the result on page 184
pnorm(98.4, mean = 98.6, sd = 0.2)
pvalue <- 2 * pnorm(98.4, mean = 98.6, sd = 0.2)
pvalue
##[1] 0.3173105

## the above testing is assume the population variance is known, but it is rarely the case when we practice.
## alomost in all the cases, we have t estiamte the population variance using the sample variance
## when we use the sample variance, we calculate the t-scores and use the t-distribution\
## t_score = (sample_mean - population_mean)/sample_sd/sqrt(sample_size)
## when we have the t_score, we can use pt() fucntion in R to calculate the probability
## This is the single sample t-test, the following result is the same as the result on page 189
pt(5.33, df = 199, lower.tail = F)
##[1] 1.324778e-07

## when we don't know the t-score in advance, we can calculate
library(MASS)
t.test(Pima.tr$bmi, mu = 30, alternative = "two.sided")
##        One Sample t-test
##
##data:  Pima.tr$bmi
##t = 5.3291, df = 199, p-value = 2.661e-07
##alternative hypothesis: true mean is not equal to 30
##95 percent confidence interval:
## 31.45521 33.16479
##sample estimates:
##mean of x 
##    32.31 

## if we need one side t-test, we should set the alternative parameter to "greater" or "less"
## the following we set "greater" because the sample mean is greater than mu
library(MASS)
t.test(Pima.tr$bmi, mu = 30, alternative = "greater")
##        One Sample t-test
##
##data:  Pima.tr$bmi
##t = 5.3291, df = 199, p-value = 1.331e-07
##alternative hypothesis: true mean is greater than 30
##95 percent confidence interval:
## 31.59367      Inf
##sample estimates:
##mean of x 
##    32.31 

## we can set the confidential level by the conf.level parameters
## notice that if we don't set the alternative parameter, its default value is "two.sided"
library(MASS)
t.test(Pima.tr$bmi, mu = 30, conf.level = 0.9)
##        One Sample t-test
##
##data:  Pima.tr$bmi
##t = 5.3291, df = 199, p-value = 2.661e-07
##alternative hypothesis: true mean is not equal to 30
##90 percent confidence interval:
## 31.59367 33.02633
##sample estimates:
##mean of x 
##    32.31

## test of the normality
## the H0 is assume the data is normally distributed
library(MASS)
shapiro.test(Pima.tr$bmi)
##        Shapiro-Wilk normality test
##
##data:  Pima.tr$bmi
##W = 0.99104, p-value = 0.2523
## the pvalue is large, so we can not reject the H0 hypothesis, the data is normally distributed, the following histogram validate
## the normal distribution too.

hist(Pima.tr$bmi, freq = FALSE, xlab = "BMI", ylab = "Density", col = "red", main = "Density histogram of BMI")

 
#############################################################################################################################
## two sample t-test
#############################################################################################################################

## to see if there is a difference between two groups
head(birthwt)
##   low age lwt race smoke ptl ht ui ftv  bwt
##85   0  19 182    2     0   0  0  1   0 2523
##86   0  33 155    3     0   0  0  0   3 2551
##87   0  20 105    1     1   0  0  0   1 2557
##88   0  21 108    1     1   0  0  1   2 2594
##89   0  18 107    1     1   0  0  1   0 2600
##91   0  21 124    3     0   0  0  0   0 2622

t.test(bwt ~ smoke, mu = 0, alternative = "two.sided", data = birthwt)
##        Welch Two Sample t-test
##
##data:  bwt by smoke
##t = 2.7299, df = 170.1, p-value = 0.007003
##alternative hypothesis: true difference in means is not equal to 0
##95 percent confidence interval:
##  78.57486 488.97860
##sample estimates:
##mean in group 0 mean in group 1 
##       3055.696        2771.919 


## paired t-test
setwd("D:\\importantbk\\learning\\data\\R_documentation\\biostatistics_with_R")

Platelet <- read.csv(file = "Platelet.txt", sep = "\t", stringsAsFactors = F)

t.test(Platelet$Before, Platelet$After, alternative = "less", paired = T)
##         Paired t-test
##
## data:  Platelet$Before and Platelet$After
## t = -4.2716, df = 10, p-value = 0.0008164
## alternative hypothesis: true difference in means is less than 0
## 95 percent confidence interval:
##       -Inf -5.913967
## sample estimates:
## mean of the differences 
##               -10.27273 

## pay close attention here, if our experiment design is paired, the data analysis should use paired t test, or the result will
## be completely different. Check the following result and compare it with the result above, the only difference is the parameter
## "paired = T", but the conclusion is almost reverse.
t.test(Platelet$Before, Platelet$After, alternative = "less")
##        Welch Two Sample t-test
##
##data:  Platelet$Before and Platelet$After
##t = -1.4164, df = 19.516, p-value = 0.08621
##alternative hypothesis: true difference in means is less than 0
##95 percent confidence interval:
##     -Inf 2.251395
##sample estimates:
##mean of x mean of y 
## 42.18182  52.45455 




#############################################################################################################################
## inference about the relationship between two binary variables
#############################################################################################################################
library(MASS)
head(birthwt)

table(birthwt$smoke, birthwt$low)
##     0  1
##  0 86 29
##  1 44 30


prop.table(table(birthwt$smoke, birthwt$low))
##           0         1
##  0 0.4550265 0.1534392
##  1 0.2328042 0.1587302


######################################
## generate the frequecies table
######################################

options(digits=3)
a=c(20,23,20,24,23,21,22,25,26,20,21,21,22,22,23,22,22,24,25,21,22,21,24,23)
b=table(a)
b1=as.vector(b)
c=prop.table(table(a))
c1=as.vector(c)
x=data.frame(b1,c1)
dimnames(x)=list(c('20','21','22','23','24','25','26'),c("频数","频率"))
x
> x
   频数   频率
20    3 0.1250
21    5 0.2083
22    6 0.2500
23    4 0.1667
24    3 0.1250
25    2 0.0833
26    1 0.0417


test <- table(birthwt$smoke, birthwt$low)
dimnames(test)=list(c("Nonsmoke","smoke"),c("Notlow","Low"))
test
##          Notlow Low
## Nonsmoke     86  29
## smoke        44  30

cor.test(test[,1], test[,2])
## Error in cor.test.default(test[, 1], test[, 2]) : 有限值的观察量不够

## so here we can not use an existing R function to perform the hypothesis testing
## we can build one by ourself


tab <- test

## calcualte the proportion and the 
u1 <- tab[1,2]/(tab[1,1] + tab[1,2])
u2 <- tab[2,2]/(tab[2,1] + tab[2,2])
u12 <- u1 - u2
SE12 <-sqrt(u1*(1-u1)/sum(tab[1,1],tab[1,2]) + u2*(1-u2)/sum(tab[2,1],tab[2,2]))
z12 <- u12/SE12
pvalue <- 2 * pnorm(z12, mean = 0, sd = 1)
## [1] 0.02855152

## the pvalue is small, so we can reject the H0 that u12 = 0 and conclude that the differect is statistically significant, that means
## the two variables are related








#############################################################################################################################
## inference regarding the linear relationship between two numerical variables
#############################################################################################################################

install.packages("mfp")
library(mfp)
data(bodyfat)
head(bodyfat)
##   case brozek siri density age weight height neck chest abdomen   hip thigh knee ankle biceps forearm wrist
## 1    1   12.6 12.3  1.0708  23 154.25  67.75 36.2  93.1    85.2  94.5  59.0 37.3  21.9   32.0    27.4  17.1
## 2    2    6.9  6.1  1.0853  22 173.25  72.25 38.5  93.6    83.0  98.7  58.7 37.3  23.4   30.5    28.9  18.2
## 3    3   24.6 25.3  1.0414  22 154.00  66.25 34.0  95.8    87.9  99.2  59.6 38.9  24.0   28.8    25.2  16.6
## 4    4   10.9 10.4  1.0751  26 184.75  72.25 37.4 101.8    86.4 101.2  60.1 37.3  22.8   32.4    29.4  18.2
## 5    5   27.8 28.7  1.0340  24 184.25  71.25 34.4  97.3   100.0 101.9  63.2 42.2  24.0   32.2    27.7  17.7
## 6    6   20.6 20.9  1.0502  24 210.25  74.75 39.0 104.5    94.4 107.8  66.0 42.0  25.6   35.7    30.6  18.8


## here we not only caculate the corelation coefficient, but also test if the corelation coefficient is statistically 
## significant

cor.test(bodyfat$siri, bodyfat$abdomen, alternative = "greater")
##        Pearson's product-moment correlation
##
##data:  bodyfat$siri and bodyfat$abdomen
##t = 22.112, df = 250, p-value < 2.2e-16
##alternative hypothesis: true correlation is greater than 0
##95 percent confidence interval:
## 0.77505 1.00000
##sample estimates:
##      cor 
##0.8134323 

## the t score is calculated as follows:
## t = r/sqrt((1-r^2)/(n-2))
## here, r = 0.8134323, n = 252 -2, and t is 22.112, this result is equal to the cor.test() function




## the following function give the cor matrix
cor(bodyfat[,c("abdomen","siri")])
##           abdomen      siri
## abdomen 1.0000000 0.8134323
## siri    0.8134323 1.0000000




#############################################################################################################################
## Analysis of Variance (ANOVA) 
#############################################################################################################################

##analysis of variance is a special case of t test
## think about this case, if we compare two groups, we can use two sample t test
## but if we want to compare three or more groups, what shall we do?
## we can still use two sample t test, but we have to do t test many times
## this situation is very common in research, for example, tumor patients have many stages, and in each stage, we may be
## interested in a variable, then our job is to compare this variable among the different stages
## so it also means the factor in the anova will have at least three levels

library(MASS)
data(Cushings)

## I want to plot the dotplot on page 233
## but surprisingly I found that the default plot function in R cannot do this
plot(Tetrahydrocortisone ~ Type, data = Cushings)
plot(Cushings$Type,Cushings$Tetrahydrocortisone,  col = "red", sub = "This is hypothetical data", xlab = "Type", ylab = "Tetrahydrocortisone")
## why the above two functions cannot give the dotplot but give the boxplot?
## the variable of the data set "Type" is factor, so the default of plot give boxplot result according to factors in "Type"
## class(Cushings$Type)
## [1] "factor"

## if we change the factor to numerical calls, then it can give the dotplot
Cushings$Typen <- as.character(Cushings$Type)
## here we should pay close attention
## we want to change the facter levels a,b,c,u to 1,2,3,4, so totally it means factor to numeric transformation
## the following code are designed to change a to 1, b to 2 and as such, but notice that, in the following code
## we treat a, b, c, u as character, if we donot transform factor to character first
## then we will get error in execution of the following code
for (i in 1:length(Cushings$Type)){
 if(Cushings[i,3] == "a"){Cushings[i,4] = 1}else if(Cushings[i,3] == "b"){
  Cushings[i,4] = 2
 }else if(Cushings[i,3] == "c"){
  Cushings[i,4] = 3
 }else if(Cushings[i,3] == "u"){
  Cushings[i,4] = 4
 }
}
## !!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!
## and pay attention to the else if function writting style again, it fools me many times!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!
## !!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!
plot(Cushings$Typen,Cushings$Tetrahydrocortisone,  col = "red", sub = "This is hypothetical data", xlab = "Type", ylab = "Tetrahydrocortisone")
## this time the plot() function give the dotplot we want


## But we still want the dot plot as the showed in many paper, the dot in this dotplot are scattered according to
## different groups, but they are not showed overlaping with each other, but we donot want them to overlap
## finally I get wo know that this plot(dotplot) should use special package or other method
install.packages("beeswarm")
install.packages("plotrix")
library(beeswarm)
library(plotrix)

require(beeswarm)
data(breast)
head(breast)
##            ER      ESR1     ERBB2 time_survival event_survival
##100.CEL.gz neg  8.372133 13.085894            39              1
##103.CEL.gz pos 10.559356  9.491683            97              0
##104.CEL.gz pos 12.299905  9.599574            11              1
##105.CEL.gz pos 10.776632  9.681747            99              0
##106.CEL.gz pos 10.505124  9.436763            40              1
##107.CEL.gz neg 10.377741  8.695576            94              0
require(plotrix)
cluster<-cluster.overplot(breast$event_survival, breast$time_survival)
png("dothist.png",width=1000,height=1000)
opar<-par(mfrow=c(3,3))

## This is plot one , in this plot, dot are overlap with each other as we showed in last example
plot (breast$event_survival, breast$time_survival, main="Multiple points on coordinate", col=as.numeric(breast$ER),xaxt="n",xlim=c(-1,2))
axis(1,at=c(0,1),labels=c("Censored","Metastasis"))

## This is plot two, in this plot, dots are not so densely overlap with each other
plot(jitter(breast$event_survival), breast$time_survival, main="Using Jitter on x-axis", col=as.numeric(breast$ER),xaxt="n",xlim=c(-0.5,1.5))
axis(1,at=c(0,1),labels=c("Censored","Metastasis"))

## This is plot three, comparing with plot two, the difference is we perform jitter() function to both x and y values
## in plot three, why? because in plot two, we perform jitter() function only to x values, it did pretty good, but
## there still many dots overlap with each other, so in plot three, we perform jitter() to both x and y values
plot(jitter(breast$event_survival), jitter(breast$time_survival), main="Using Jitter on x and y-axis", col=as.numeric(breast$ER),xaxt="n",xlim=c(-0.5,1.5))
axis(1,at=c(0,1),labels=c("Censored","Metastasis"))

## or we can use factor parameter in jitter() function to make the dots scatter more dispersedly
plot(rep(c(1,5,10),each=5),c(jitter(rep(100,5),factor=1),jitter(rep(100,5),factor=5), jitter(rep(100,5),factor=10)),col=c("red","blue","green","gray","black"),xlim=c(-2,13),xlab="", ylab="y",xaxt="n",main="jitter(rep(100,5)) with different factor")
axis(1,at=c(1,5,10),labels=c(paste("factor=",c(1,5,10),sep="")))


## This is plot four, this is so called sunflower plot. sunflowerplot is used to show the number of dots via the number of
## of petals at the same coordinates. But in this example, it seems not so readable.
## sunflowerplot函数。它的目的是用花瓣数目多少来显示落在同一坐标上的点的数目。但是从中左图看来，点多的时候效果并非总是那么好。
sunflowerplot(breast$event_survival, breast$time_survival, main="Using Sunflowers",xaxt="n",xlim=c(-0.5,1.5))
axis(1,at=c(0,1),labels=c("Censored","Metastasis"))

## This is plot five, this plot use cluster.overplot to cluster values first, the function enclosed in plotrix package
plot(cluster, main="Using cluster.overplot",col=as.numeric(breast$ER),xaxt="n",xlim=c(-0.5,1.5))
axis(1,at=c(0,1),labels=c("Censored","Metastasis"))

## This is plot six,count.overplot() function in plotrix package
count.overplot(jitter(breast$event_survival), jitter(breast$time_survival), main="Using cout.overplot", col=as.numeric(breast$ER),xaxt="n")
axis(1,at=c(0,1),labels=c("Censored","Metastasis"))

## This is plot seven, this is so called size plot, sizeplot() function in plotrix package
sizeplot(breast$event_survival, breast$time_survival, main="Using sizeplot",col=as.numeric(breast$ER), xaxt="n",xlim=c(-0.5,1.5))
axis(1,at=c(0,1),labels=c("Censored","Metastasis"))

## The above three plots(five, six, seven) all use the functions in plotrix package
## 分别是cluster.overplot, count.overplot, sizeplot。这三个函数的效果如图中及下靠左的两个。cluster.overplot的方法类似抖散，count overplot的方法是使用数字来显示落在同一坐标上的点的数目，sizeplot的方法是使用不同大小是点来显示落在同一坐标上的点的数目。从效果来看，点多的时候效果也并非理想。而上一次提到过的蜂群图似乎是解决这一问题的较佳方案。
##我们得出结论，在点数不同的情况下，使用plotrix包及sunflowerplot是不错的。但点数较多的情况下，还是使用jitter和beeswarm较为稳妥。


## This is plot eight, this plot is exactly what we want, this is so called beeswarm plot
beeswarm(time_survival ~ event_survival, data = breast, method = 'swarm', pch = 16, pwcol = as.numeric(ER), xlab = '', ylab = 'Follow-up time (months)', labels = c('Censored', 'Metastasis'))
dev.off()
##quartz
##     2
par(opar)

## we can also use ggplot to get the beeswarm plot
require(beeswarm)
data(breast)
library(ggplot2)
p<-ggplot(breast, aes(event_survival,time_survival))
print(p+geom_jitter(aes(color=ER))+scale_colour_manual(value = c("black", "red"))+scale_x_continuous(breaks = c(0:1), labels = c("Censored", "Metastasis")))



##还有一种图，名称为Engelmann-Hecker-Plot， 由plotrix的ehplot来实现
data(iris)
library(plotrix)
ehplot(iris$Sepal.Length, iris$Species, intervals=20, cex=1.8, pch=20, main="pch=20")
ehplot(iris$Sepal.Width, iris$Species, intervals=20, box=TRUE, median=FALSE, main="box=TRUE")
ehplot(iris$Petal.Length, iris$Species, pch=17, col="red", log=TRUE, main="pch=17")
ehplot(iris$Petal.Length, iris$Species, offset=0.06, pch=as.numeric(iris$Species), main="pch=as.numeric(iris$Species)")
rnd <- sample(150)
plen <- iris$Petal.Length[rnd]
pwid <- abs(rnorm(150, 1.2))
spec <- iris$Species[rnd]
ehplot(plen, spec, pch=19, cex=pwid, col=rainbow(3, alpha=0.6)[as.numeric(spec)], main="cex and col changes")




## SSb means the variation between groups
SSb = n1*(y1 - y)^2 + n2*(y2 - y)^2 + n3*(y3 - y)^2 + ... + ni*(yi - y)^2
## i means the number of groups
## n1 means the observations in group1
## y1 means the mean of groups
## y means the mean of all values


## SSw means the variation with the groups
SSw = [(y11-y1)^2 + (y12 - y1)^2 + (y13 - y1)^2 + ... + (y1j - y1)^2] 
+[(y21-y2)^2 + (y22 - y2)^2 + (y23 - y2)^2 + ... + (y2j - y2)^2]
+[(y31-y3)^2 + (y32 - y3)^2 + (y33 - y3)^2 + ... + (y3j - y3)^2]
+ ...
+[(yi1-yi)^2 + (yi2 - yi)^2 + (yi3 - yi)^2 + ... + (yij - yi)^2]


## SS means the total variation

SSw = [(y11-y)^2 + (y12 - y)^2 + (y13 - y)^2 + ... + (y1j - y)^2] 
+[(y21-y)^2 + (y22 - y)^2 + (y23 - y)^2 + ... + (y2j - y)^2]
+[(y31-y)^2 + (y32 - y)^2 + (y33 - y)^2 + ... + (y3j - y)^2]
+ ...
+[(yi1-y)^2 + (yi2 - y)^2 + (yi3 - y)^2 + ... + (yij - y)^2]


## In other words, the total variation can be attributed partly to the variation within groups and partly to the 
## variation between groups. SSb is interpreted as the part of total variation SS that is associated with (and can 
## be explained by) the factor variable X (groups). In contrast, SSw is regarded as the unexplained part of total 
## variation and is regarded as random.


## if the interest variable does not depend on the factor we considered, we expect the group-specific averages to be the 
## same. That is, we expect the mean in each group lies around the mean of total and any observed variation of group mean
## around the total mean to be due to chance alone. On the other hand, if there is a substantial difference in
## the interest variable depending on the factor we considered, then we would expect the variation between groups to be 
## large. We examine the amount of between-groups variation relative to the variation within groups (which occurs randomly).

## Let us denote the overall population mean of Y as μ and group-specific population means as μ1 , . . . , μ4 . Then we can
## express the null hypothesis of no difference in means between the groups as
## H0 : u1 = u2 = u3 = u4 = ... = ui = u

## The process of evaluating hypotheses regarding the group means of mul- tiple populations is called the Analysis of 
## Variance (ANOVA). Since we are only considering one factor only, this method is specifically called one- way ANOVA. 
## The test statistic for examining the null hypothesis is called F -statistic (more specifically, ANOVA F -statistic) 
## and is defined as
###########################
## F = SSb/(k-1)/SSw/(n-k)
###########################
## k is the number of groups, n is the total sample size
## The numerator is called the mean square for groups, and the denominator is called the mean square error (MSE)

## Note that the above test statistic is based on comparing the variation between groups (which is explained by the 
## factor) and the variation within groups (which is unexplained and random). When the group means are substantially
## different, and their variation is relatively large compared to the random varia- tions within groups, the value of 
## the F statistic becomes large.

## We denote the observed value of the F -statistic as f . If the null hypothesis is true, then the test statistic F 
##  has an F-distribution.


## For the one-way ANOVA, the F-statistic has F(df1 = k − 1, df2 = n − k) distribution under the null hypothesis 
## (i.e., assuming that the null hypothesis is true). Here, df1 = k − 1, which is the number of groups minus 1, is 
## called the numerator degrees of freedom, and df2 = n − k, which is the sample size minus the number of groups, is 
## called the denominator degrees of freedom. The underlying assumption here is that the observations in each group 
## are IID (e.g., obtained through SRS) and have a normal distribution. The results are not sensitive to the 
## normality assumption as long as the sample sizes are large enough for the CLT to hold.
## Additionally, the underlying assumption of the ANOVA method discussed here is that all groups have the same 
## population variance, σ2, which is unknown.



## if we have the f value, we can use pf() function to calculate the pvalue
pf(3.2, df1 = 3, df2 = 23, lower.tail = FALSE)
## [1] 0.04226148


library(MASS)
data(Cushings)
aov1.out <- aov(Tetrahydrocortisone ~ Type, data = Cushings)
summary(aov1.out)
##            Df Sum Sq Mean Sq F value Pr(>F)  
##Type         3  893.5  297.84   3.226 0.0412 *
##Residuals   23 2123.6   92.33                 
##---
##Signif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1

## here The first column shows the degrees of freedom (Df), which are k − 1 = 3 and n − k = 23, respec- tively. 
## The values of the second column, labeled Sum Sq, are the between-groups and within-groups variations: SSb = 893.5 
## and SSw = 2123.6. The observed value of F -statistic is f = 3.2 given under the column labeled F value. The resulting 
## p-value is then 0.04. 


## About the assumptions 










library(MASS)
data(genotype)
aov2.out <- aov(Wt ~ Mother + Litter, data = genotype)
summary(aov2.out)
##            Df Sum Sq Mean Sq F value  Pr(>F)   
##Mother       3    772  257.20   4.254 0.00905 **
##Litter       3     64   21.21   0.351 0.78870   
##Residuals   54   3265   60.46                   
##---
##Signif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1




library(MASS)
data(genotype)
aov2.int.out <- aov(Wt ~ Mother * Litter, data = genotype)
summary(aov2.int.out)
##              Df Sum Sq Mean Sq F value  Pr(>F)   
##Mother         3  771.6  257.20   4.742 0.00587 **
##Litter         3   63.6   21.21   0.391 0.76000   
##Mother:Litter  9  824.1   91.56   1.688 0.12005   
##Residuals     45 2440.8   54.24                   
##---
##Signif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1












