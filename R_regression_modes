getwd()
setwd("D:\\importantbk\\learning\\data\\R_documentation\\biostatistics_with_R")
list.files()
tab <- read.csv(file = "saltBP.txt", stringsAsFactors = F)

test_fit <- lm(BP ~ saltLevel, data = tab)
test_fit
##Call:
##lm(formula = BP ~ saltLevel, data = tab)
##
##Coefficients:
##(Intercept)    saltLevel  
##    133.173        6.256 

summary(test_fit)
##
##Call:
##lm(formula = BP ~ saltLevel, data = tab)
##
##Residuals:
##    Min      1Q  Median      3Q     Max 
##-8.2990 -3.5627  0.6873  3.2110  5.5910 
##
##Coefficients:
##            Estimate Std. Error t value Pr(>|t|)    
##(Intercept)  133.173      1.007 132.222  < 2e-16 ***
##saltLevel      6.256      1.593   3.929 0.000672 ***
##---
##Signif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1
##
##Residual standard error: 3.901 on 23 degrees of freedom
##Multiple R-squared:  0.4016,	Adjusted R-squared:  0.3756 
##F-statistic: 15.43 on 1 and 23 DF,  p-value: 0.0006716


## Residual Sum of Squares (RSS)
## RSS = sum(e1^2 + e2^2 + e3^2 + ... + ei^2)

## y = a + b * x
## For b is a point estimation of the slope of the regression line, to establish a confidence interval, we have to get
## the SE and with the help of SE, we can even perform the hypothesis testing
## SEb = sqrt(RSS/(n - 2))/sqrt(sum((x1 - x)^2 + (x2 - x)^2 + (x3 - x)^2 + ... + (xi - x)^2))
## n is the sample size

tab$exp <- 133.173 + 6.256 * tab$saltLevel
tab$res <- tab$BP - tab$exp
tab_RSS <- sum(tab$res^2)
tab_RSS
##[1] 349.9796
tab_var <- tab$saltLevel - mean(tab$saltLevel)

tab_SEb <- sqrt(tab_RSS/(25-2))/sqrt(sum(tab_var^2))
tab_SEb
##[1] 1.592509
## the result is the same as the result on page 261 and lm() function result in R

t_score_b <- 6.25/tab_SEb
t_score_b
##[1] 3.924625
## the result is the same as the result on page 263 and lm() function result in R
## this  t-distribution has a df of 25 - 2 = 23, the pvalue
pt(3.93, df = 23, lower.tail = F)
## 0.000334624
## the final pvalue = 0.000334624 * 2 = 0.000669248
## the result is the same as the result on page 263 and lm() function result in R

head(tab)
##      BP salt saltLevel     exp    res
##1 132.19 1.55         0 133.173 -0.983
##2 131.84 1.13         0 133.173 -1.333
##3 133.86 4.88         0 133.173  0.687
##4 135.08 4.11         0 133.173  1.907
##5 129.85 1.55         0 133.173 -3.323
##6 136.84 4.69         0 133.173  3.667


######################################################
## The above example is very limited,because we use a binary variable as an explanatory , it is rarely the case in practice
## Following we are going to use a numerical variable as explanatory variable

getwd()
setwd("D:\\importantbk\\learning\\data\\R_documentation\\biostatistics_with_R")
list.files()
tab <- read.csv(file = "saltBP.txt", stringsAsFactors = F)

## the coefficient of the two variables is large enough and statistically significant
## the Pearson's product-moment correlation is positive means the two variables are positively correlated
cor.test(tab$BP, tab$salt)
##
##        Pearson's product-moment correlation
##
##data:  tab$BP and tab$salt
##t = 7.3888, df = 23, p-value = 1.631e-07
##alternative hypothesis: true correlation is not equal to 0
##95 percent confidence interval:
## 0.6636134 0.9267667
##sample estimates:
##      cor 
##0.8387992 

fit <- lm(BP ~ salt, data = tab)
summary(fit)
##
##Call:
##lm(formula = BP ~ salt, data = tab)
##
##Residuals:
##    Min      1Q  Median      3Q     Max 
##-5.0388 -1.6755  0.3662  1.8824  5.3443 
##
##Coefficients:
##            Estimate Std. Error t value Pr(>|t|)    
##(Intercept)  128.616      1.102 116.723  < 2e-16 ***
##salt           1.197      0.162   7.389 1.63e-07 ***
##---
##Signif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1
##
##Residual standard error: 2.745 on 23 degrees of freedom
##Multiple R-squared:  0.7036,    Adjusted R-squared:  0.6907 
##F-statistic: 54.59 on 1 and 23 DF,  p-value: 1.631e-07

## the essence of linear model fitting is to find the a and b in  y = a + bx
## how to calculate these two values

## b = r * Sy/Sx
## r is the correlation  coefficient
## Sy is the standard deviation of response Y 
## Sx is the standard deviation of explanatory X
## in our example:
b <- cor(tab$BP, tab$salt) * sd(tab$BP) / sd(tab$salt)
b
## [1] 1.196894
## the result is same as the lm() function result in R and the result on page 267


## a = mean(y) - b * mean(x)
## b is the slope of the regression line, we have calculated it before hand
## in our example
a <- mean(tab$BP) - b * mean(tab$salt)
a
## [1] 128.6164
## the result is same as the lm() function result in R and the result on page 267

## so the regression mode is y = 128.6164 + 1.196894 * x

## the following is calculate the significance of b value
tab$exp <- 128.6164 + 1.196894 * tab$salt
tab$res <- tab$BP - tab$exp
tab_RSS <- sum(tab$res^2)
tab_RSS
##[1] 173.3528
tab_var <- tab$salt - mean(tab$salt)

tab_SEb <- sqrt(tab_RSS/(25-2))/sqrt(sum(tab_var^2))
tab_SEb
##[1] 0.1619886
## the result is the same as the result on page 270 and lm() function result in R

t_score_b <- 1.196894/tab_SEb
t_score_b
##[1] 7.388754
## the result is the same as the result on page 263 and lm() function result in R
## this  t-distribution has a df of 25 - 2 = 23, the pvalue
pt(7.388754, df = 23, lower.tail = F)
## 8.156051e-08
## the final pvalue = 8.156051e-08 * 2 = 1.63121e-07
## the result is exacatly the same as the result on page 270 and lm() function result in R



######################################################
## model goodness assessing
## model goodness assessing is a big issue, here we first focused on the simple R squared method
## and then focused on the ROC method

## RSS(Residual Sum of Squares) is interpreted as the unexplained variation in the response variable using the regression model
## or we can understand the RSS as the lack of fit of the linear regression model
## R squared is the measure of goodness of fit, that is how well our model represents the observed data and explains the variation
## in the response variable.
## the R squared value is between 0 and 1, the better the model fits the data, the higher its R squared value is
## TSS(Total Sum of Squares) is calculated as the squared deviations of each observed value of the response variable yi from its
## sample mean mean(y)
## TSS = (y1 - mean(y))^2 + (y2 - mean(y))^2 + (y3 - mean(y))^2 + ... + (yi - mean(y))^2
######################################
## R squared = 1 - RSS/TSS
######################################
## interpretation of R squared value(R2), for example, if R2 = 0.7, it means 70% of the total variation in the response variable
## can bu explained by the explanatory variable. The remaining 30% of the total variation can not be explained by this model
## and is regarded as random


## For simple linear regression models with one numerical explannatory variable, R2 is equal to the square of the 
## corelation coefficient r
######################################
## R squared = r^2
######################################

##  in our example
tab$exp <- 128.6164 + 1.196894 * tab$salt
tab$res <- tab$BP - tab$exp
tab_RSS <- sum(tab$res^2)
tab_RSS
##[1] 173.3528
tab_TSS <- sum((tab$BP - mean(tab$BP))^2)
##[1] 584.8298

tab_R2 <- 1 - tab_RSS/584.8298
tab_R2
##[1] 0.7035842
## the result is the same as the lm() function in R

cor(tab$BP, tab$salt)^2
##[1] 0.7035842
## which is alsoa way of calculating R square

## plase be very caution that even if we have a very large R2 value, it only means this model did good job in prediction  in the 
## observed dataset, it does not translateto better predictions for other dataset or other individuals that dose not included in
## our sample. Sometimes even if we have a small R2 value, it still better than we do not use any model for prediction

## the three assumptions in linear regression fitting
## 1. Linearity
##       the relationship between explanatory variable and response variable is linear, this linearity can be showned in scatter plot
##       If the linearity assumption dose not hold, we might still be able to use linear regression models after transforming the 
##       original variables. Common transformation are logarithm(usually for response variable), square root and sqaure(usually
##       for the predictors), the regression model will be like this:
##       y = a + b1 * x + b2 * x^2 + e       
##       pay attention  here, the role of x^2 is to capture possible nonlinearity in the relationship between Y and X. In this case,
##       the null hypothesis H0 : b2 = 0 indicates that the relationship in not quadratic 

## 2. Independence
##       the individuals in our sample are not related to each other and they are mutually independence
##       or we do not obtain multiple observations from the same individual
##       But sometimes we selected the unrelated subjects and obtain multiple measurements(over a period of time)
##       under this circumstances, we will use longitudinal models

## 3. Constant Variance and Normality
##       we assume the error term e is normally distributed with mean 0.   e ~ N(0, σ^2)
##       Minor deviations from  normality will not have a substantial impacton the results as long as the sample size is large
##       this assumption means that we expect the variation of the actual values of response variable around the regression
##       line remains the same regardless of the explanatory value. This is called the constant variance assumption, which
##       is also known as the homoscedasticity assumption. To check the validity of these assumptions. we examine the 
##       residuals e that are observed values of the random variable e.
##       This examine can be performed with the help of residual plot. 
##       The horizontal line represents the regression line, the residues are scattered randomly around the horizontal line
##       at zero without any pattern. The solid line shows the overall pattern of the residuals. This line should remain close
##       to the horizontal line. It is very important that we do not see any nonrandom pattern in the residual plot. For example
##       we should not see small variations 
##


install.packages("car")
library(car)
crplots()























## install.packages("mfp", dependencies = TRUE)
library(mfp)
data(bodyfat)


fit <- lm(siri ~ height, data = bodyfat)

fit
##Call:
##lm(formula = siri ~ height, data = bodyfat)
##
##Coefficients:
##(Intercept)       height  
##    33.4945      -0.2045 

summary(fit)
##Call:
##lm(formula = siri ~ height, data = bodyfat)
##
##Residuals:
##     Min       1Q   Median       3Q      Max 
##-19.5902  -6.7124   0.3966   6.0716  27.0919 
##
##Coefficients:
##            Estimate Std. Error t value Pr(>|t|)   
##(Intercept)  33.4945    10.1096   3.313  0.00106 **
##height       -0.2045     0.1439  -1.421  0.15664   
##---
##Signif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1
##
##Residual standard error: 8.352 on 250 degrees of freedom
##Multiple R-squared:  0.008009,	Adjusted R-squared:  0.004041 
##F-statistic: 2.019 on 1 and 250 DF,  p-value: 0.1566

names(fit)
##[1] "coefficients"  "residuals"     "effects"       "rank"          "fitted.values" "assign"        "qr"            "df.residual"   "xlevels"      
##[10] "call"          "terms"         "model" 

fit$coefficients
##(Intercept)      height 
## 33.4944938  -0.2044753

fit$fitted.values[1:5]
##       1        2        3        4        5 
##19.64129 18.72115 19.94800 18.72115 18.92563 

fit$residuals[1:5]
##         1          2          3          4          5 
## -7.341291 -12.621152   5.351996  -8.321152   9.774373 

plot(bodyfat$height, bodyfat$siri, main = "Scatterplot for Percent Body Fat by Height", xlab = "Height", ylab = "Percent Body Fat")
abline(fit)









multReg <- lm(siri ~ height + abdomen, data = bodyfat)

summary(multReg)
##Call:
##lm(formula = siri ~ height + abdomen, data = bodyfat)
##
##Residuals:
##     Min       1Q   Median       3Q      Max 
##-18.8513  -3.4825  -0.0156   3.0949  11.1633 
##
##Coefficients:
##             Estimate Std. Error t value Pr(>|t|)    
##(Intercept) -14.31075    6.04265  -2.368   0.0186 *  
##height       -0.37053    0.08122  -4.562 7.95e-06 ***
##abdomen       0.64236    0.02759  23.283  < 2e-16 ***
##---
##Signif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1
##
##Residual standard error: 4.695 on 249 degrees of freedom
##Multiple R-squared:  0.6878,	Adjusted R-squared:  0.6853 
##F-statistic: 274.2 on 2 and 249 DF,  p-value: < 2.2e-16





















