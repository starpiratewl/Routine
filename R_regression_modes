getwd()
setwd("D:\\importantbk\\learning\\data\\R_documentation\\biostatistics_with_R")
list.files()
tab <- read.csv(file = "saltBP.txt", stringsAsFactors = F)

test_fit <- lm(BP ~ saltLevel, data = tab)
test_fit
##Call:
##lm(formula = BP ~ saltLevel, data = tab)
##
##Coefficients:
##(Intercept)    saltLevel  
##    133.173        6.256 

summary(test_fit)
##
##Call:
##lm(formula = BP ~ saltLevel, data = tab)
##
##Residuals:
##    Min      1Q  Median      3Q     Max 
##-8.2990 -3.5627  0.6873  3.2110  5.5910 
##
##Coefficients:
##            Estimate Std. Error t value Pr(>|t|)    
##(Intercept)  133.173      1.007 132.222  < 2e-16 ***
##saltLevel      6.256      1.593   3.929 0.000672 ***
##---
##Signif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1
##
##Residual standard error: 3.901 on 23 degrees of freedom
##Multiple R-squared:  0.4016,	Adjusted R-squared:  0.3756 
##F-statistic: 15.43 on 1 and 23 DF,  p-value: 0.0006716


## Residual Sum of Squares (RSS)
## RSS = sum(e1^2 + e2^2 + e3^2 + ... + ei^2)

## y = a + b * x
## For b is a point estimation of the slope of the regression line, to establish a confidence interval, we have to get
## the SE and with the help of SE, we can even perform the hypothesis testing
## SEb = sqrt(RSS/(n - 2))/sqrt(sum((x1 - x)^2 + (x2 - x)^2 + (x3 - x)^2 + ... + (xi - x)^2))
## n is the sample size

tab$exp <- 133.173 + 6.256 * tab$saltLevel
tab$res <- tab$BP - tab$exp
tab_RSS <- sum(tab$res^2)
tab_RSS
##[1] 349.9796
tab_var <- tab$saltLevel - mean(tab$saltLevel)

tab_SEb <- sqrt(tab_RSS/(25-2))/sqrt(sum(tab_var^2))
tab_SEb
##[1] 1.592509
## the result is the same as the result on page 261 and lm() function result in R

t_score_b <- 6.25/tab_SEb
t_score_b
##[1] 3.924625
## the result is the same as the result on page 263 and lm() function result in R
## this  t-distribution has a df of 25 - 2 = 23, the pvalue
pt(3.93, df = 23, lower.tail = F)
## 0.000334624
## the final pvalue = 0.000334624 * 2 = 0.000669248
## the result is the same as the result on page 263 and lm() function result in R

head(tab)
##      BP salt saltLevel     exp    res
##1 132.19 1.55         0 133.173 -0.983
##2 131.84 1.13         0 133.173 -1.333
##3 133.86 4.88         0 133.173  0.687
##4 135.08 4.11         0 133.173  1.907
##5 129.85 1.55         0 133.173 -3.323
##6 136.84 4.69         0 133.173  3.667


######################################################
## The above example is very limited,because we use a binary variable as an explanatory , it is rarely the case in practice
## Following we are going to use a numerical variable as explanatory variable

getwd()
setwd("D:\\importantbk\\learning\\data\\R_documentation\\biostatistics_with_R")
list.files()
tab <- read.csv(file = "saltBP.txt", stringsAsFactors = F)

## the coefficient of the two variables is large enough and statistically significant
## the Pearson's product-moment correlation is positive means the two variables are positively correlated
cor.test(tab$BP, tab$salt)
##
##        Pearson's product-moment correlation
##
##data:  tab$BP and tab$salt
##t = 7.3888, df = 23, p-value = 1.631e-07
##alternative hypothesis: true correlation is not equal to 0
##95 percent confidence interval:
## 0.6636134 0.9267667
##sample estimates:
##      cor 
##0.8387992 

fit <- lm(BP ~ salt, data = tab)
summary(fit)
##
##Call:
##lm(formula = BP ~ salt, data = tab)
##
##Residuals:
##    Min      1Q  Median      3Q     Max 
##-5.0388 -1.6755  0.3662  1.8824  5.3443 
##
##Coefficients:
##            Estimate Std. Error t value Pr(>|t|)    
##(Intercept)  128.616      1.102 116.723  < 2e-16 ***
##salt           1.197      0.162   7.389 1.63e-07 ***
##---
##Signif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1
##
##Residual standard error: 2.745 on 23 degrees of freedom
##Multiple R-squared:  0.7036,    Adjusted R-squared:  0.6907 
##F-statistic: 54.59 on 1 and 23 DF,  p-value: 1.631e-07

## the essence of linear model fitting is to find the a and b in  y = a + bx
## how to calculate these two values

## b = r * Sy/Sx
## r is the correlation  coefficient
## Sy is the standard deviation of response Y 
## Sx is the standard deviation of explanatory X
## in our example:
b <- cor(tab$BP, tab$salt) * sd(tab$BP) / sd(tab$salt)
b
## [1] 1.196894
## the result is same as the lm() function result in R and the result on page 267


## a = mean(y) - b * mean(x)
## b is the slope of the regression line, we have calculated it before hand
## in our example
a <- mean(tab$BP) - b * mean(tab$salt)
a
## [1] 128.6164
## the result is same as the lm() function result in R and the result on page 267

## so the regression mode is y = 128.6164 + 1.196894 * x

## the following is calculate the significance of b value
tab$exp <- 128.6164 + 1.196894 * tab$salt
tab$res <- tab$BP - tab$exp
tab_RSS <- sum(tab$res^2)
tab_RSS
##[1] 173.3528
tab_var <- tab$salt - mean(tab$salt)

tab_SEb <- sqrt(tab_RSS/(25-2))/sqrt(sum(tab_var^2))
tab_SEb
##[1] 0.1619886
## the result is the same as the result on page 270 and lm() function result in R

t_score_b <- 1.196894/tab_SEb
t_score_b
##[1] 7.388754
## the result is the same as the result on page 263 and lm() function result in R
## this  t-distribution has a df of 25 - 2 = 23, the pvalue
pt(7.388754, df = 23, lower.tail = F)
## 8.156051e-08
## the final pvalue = 8.156051e-08 * 2 = 1.63121e-07
## the result is exacatly the same as the result on page 270 and lm() function result in R



######################################################
## model goodness assessing
## model goodness assessing is a big issue, here we first focused on the simple R squared method
## and then focused on the ROC method

## RSS(Residual Sum of Squares) is interpreted as the unexplained variation in the response variable using the regression model
## or we can understand the RSS as the lack of fit of the linear regression model
## R squared is the measure of goodness of fit, that is how well our model represents the observed data and explains the variation
## in the response variable.
## the R squared value is between 0 and 1, the better the model fits the data, the higher its R squared value is
## TSS(Total Sum of Squares) is calculated as the squared deviations of each observed value of the response variable yi from its
## sample mean mean(y)
## TSS = (y1 - mean(y))^2 + (y2 - mean(y))^2 + (y3 - mean(y))^2 + ... + (yi - mean(y))^2
######################################
## R squared = 1 - RSS/TSS
######################################
## interpretation of R squared value(R2), for example, if R2 = 0.7, it means 70% of the total variation in the response variable
## can bu explained by the explanatory variable. The remaining 30% of the total variation can not be explained by this model
## and is regarded as random


## For simple linear regression models with one numerical explannatory variable, R2 is equal to the square of the 
## corelation coefficient r
######################################
## R squared = r^2
######################################

##  in our example
tab$exp <- 128.6164 + 1.196894 * tab$salt
tab$res <- tab$BP - tab$exp
tab_RSS <- sum(tab$res^2)
tab_RSS
##[1] 173.3528
tab_TSS <- sum((tab$BP - mean(tab$BP))^2)
##[1] 584.8298

tab_R2 <- 1 - tab_RSS/584.8298
tab_R2
##[1] 0.7035842
## the result is the same as the lm() function in R

cor(tab$BP, tab$salt)^2
##[1] 0.7035842
## which is alsoa way of calculating R square

## plase be very caution that even if we have a very large R2 value, it only means this model did good job in prediction  in the 
## observed dataset, it does not translateto better predictions for other dataset or other individuals that dose not included in
## our sample. Sometimes even if we have a small R2 value, it still better than we do not use any model for prediction

## the three assumptions in linear regression fitting
## 1. Linearity
##       the relationship between explanatory variable and response variable is linear, this linearity can be showned in scatter plot
##       If the linearity assumption dose not hold, we might still be able to use linear regression models after transforming the 
##       original variables. Common transformation are logarithm(usually for response variable), square root and sqaure(usually
##       for the predictors), the regression model will be like this:
##       y = a + b1 * x + b2 * x^2 + e       
##       pay attention  here, the role of x^2 is to capture possible nonlinearity in the relationship between Y and X. In this case,
##       the null hypothesis H0 : b2 = 0 indicates that the relationship in not quadratic 

## 2. Independence
##       the individuals in our sample are not related to each other and they are mutually independence
##       or we do not obtain multiple observations from the same individual
##       But sometimes we selected the unrelated subjects and obtain multiple measurements(over a period of time)
##       under this circumstances, we will use longitudinal models

## 3. Constant Variance and Normality
##       we assume the error term e is normally distributed with mean 0.   e ~ N(0, σ^2)
##       Minor deviations from  normality will not have a substantial impacton the results as long as the sample size is large
##       this assumption means that we expect the variation of the actual values of response variable around the regression
##       line remains the same regardless of the explanatory value. This is called the constant variance assumption, which
##       is also known as the homoscedasticity assumption. To check the validity of these assumptions. we examine the 
##       residuals e that are observed values of the random variable e.
##       This examine can be performed with the help of residual plot. 
##       The dash line represents the regression line, the residues are scattered randomly around the dash line
##       without any pattern. The solid line shows the overall pattern of the residuals. This line should remain close
##       to the horizontal line. It is very important that we do not see any nonrandom pattern in the residual plot. 
##       For example we should not see small variations around the dash line in one region and high variations in another region
##       The following codes give the residual plot to check for the residual distribution

install.packages("car")
library(car)
crPlots(fit <- lm(BP ~ salt, data = tab))

##       When the constant variance assumption does hot hold, we can sometimes stabilize the variance using simple
##       transformations of the response variable so the variance becomes approximately constant
##       For example, instead of Y, we could use sqrt(Y)(usually Y is a count variable) or log(Y) in regression model
##       Another strategy is to use weighted least squares instead of the standard least squares approach

## SEe = sqrt(RSS/(n- 2))
## n is the sample size
## SEe is the regression standard error
## so we can rewrite the SEb formulaa as :
## SEb = SEe/sqrt(sum((x1 - mean(x))^2 + (x2 - mean(x))^2 + (x3 - mean(x))^2 + ... + (xi - mean(x))^2))















######################################################
## Multiple linear Regression
## If our objective is to predict unknown values of the reponse variable, we might be able to improve prediction accuracy by
## including multiple predictors in the linear regression model. Models with multiple explanatory variables or predictors
## are call ed multiple linear regression models

## In practice, we need to  specify our objective for building linear regression model clearly!!
## If our objective is to examine possible relationships between the response variable and one or more explanatory variables,
##    we should specify our hypothesis prior to our analysis. In this case, our decision to include an explanatory variable in
##    the model must be hypothesis-driven and based onour domain knowledge. When testing a hypothesis, we should avoid finding
##    our model through exploring all possible combinations of variables available to us.
## If our objective is to predict the unknown values of the response variable, we could use our domain knowledge to identify
##    a set of promising predictors and then explore all possible combinations of these variables until we find a model
##    that provide the best predictions

## MSE ( mean squared error)
## MSE = 1/N * sum((y1 - y1p)^2 + (y2 - y2p)^2 + (y3 - y3p)^2 + ... + (yi - yip)^2)
## y1...yi are the observed values
## y1p ... yip are the predicted values

## to perform this calculation, in practice , we can divide our dataset to test set and training set.
## we use the training set to fit the models
## we use the test set to predict the response variable and compare the observed value and predicted value

## in multiple regression models
## R2 = 1 - RSS/TSS
## SEe = sqrt(RSS/(n - p - 1))
## pay attention here, n is the sample size, p is the number of explanatory variables in the model
## this is different from the simple regression model :
## SEe = sqrt(RSS/(n- 2))

## for each regression coefficient bj, we can calculate the SEbj along with its point estiamte bj, similar to simple regression
## model, we can perform the hypothesis testing using t distribution:
## tj = bj / SEbj

library(MASS)
data(birthwt)
mulreg_test <-lm(bwt ~ age + smoke, data = birthwt)
summary(mulreg_test)
##
##Call:
##lm(formula = bwt ~ age + smoke, data = birthwt)
##
##Residuals:
##     Min       1Q   Median       3Q      Max 
##-2119.98  -442.66    52.92   532.38  1690.74 
##
##Coefficients:
##            Estimate Std. Error t value Pr(>|t|)    
##(Intercept) 2791.224    240.950  11.584   <2e-16 ***
##age           11.290      9.881   1.143    0.255    
##smoke       -278.356    106.987  -2.602    0.010 *  
##---
##Signif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1
##
##Residual standard error: 717.2 on 186 degrees of freedom
##Multiple R-squared:  0.04299,   Adjusted R-squared:  0.0327 
##F-statistic: 4.177 on 2 and 186 DF,  p-value: 0.0168

## y = 2791.224 + 11.290 * age - 278.356 * smoke

## let us calculate the relative parameters to validate the result of lm() function in R
birthwt$bwt_pre <- 2791.224 + 11.290 * birthwt$age - 278.356 * birthwt$smoke
bwt_RSS <- sum((birthwt$bwt - birthwt$bwt_pre)^2)
bwt_TSS <- sum((birthwt$bwt - mean(birthwt$bwt))^2)
bwt_R2 <- 1 - bwt_RSS/bwt_TSS
bwt_R2
##[1] 0.04298673
## this result is the same as the lm() result in R

SEb1 = sqrt(bwt_RSS/(189 -2 -1)) / sqrt(sum((birthwt$age - mean(birthwt$age))^2))
SEb2 = sqrt(bwt_RSS/(189 -2 -1)) / sqrt(sum((birthwt$smoke - mean(birthwt$smoke))^2))

SEb1
##[1] 9.871659
SEb2
##[1] 106.8815
## this result is the same as the lm() result in R

tb1 <- 11.290/SEb1
tb2 <- - 278.356/SEb2
tb1
##[1] 1.143678
tb2
##[1] -2.604341
## this result is the same as the lm() result in R

pvalueb1 <- pt(tb1, df = 188, lower.tail = F) * 2
## pay attention here, the t value is positive, so we use the lower.tail = F
pvalueb2 <- pt(tb2, df = 188, lower.tail = T) * 2
## pay attention here, the t value is negative, so we use the lower.tail = T

pvalueb1
##[1] 0.2542114
pvalueb2
##[1] 0.009940866
## this result is the same as the lm() result in R

sd(birthwt$bwt - birthwt$bwt_pre)
##[1] 713.3689
## this result is different from the result in the lm() function in R, why ?
## because the df setting, here we should use the df n - p - 1

birthwt_error <- birthwt$bwt - birthwt$bwt_pre
sqrt(sum((birthwt_error - mean(birthwt_error))^2) / (189 - 2 - 1))
##[1] 717.1939
## this result is the same as the lm() result in R



fit_smoke <- lm(bwt ~ age, data = birthwt[birthwt$smoke == 1,])
fit_nonsmoke <- lm(bwt ~ age, data = birthwt[birthwt$smoke == 0,])

plot(birthwt[birthwt$smoke == 1,]$age, birthwt[birthwt$smoke == 1,]$bwt, type = "p", pch = 15, col = "red", main = "Smoke vs Nonsmoke", xlab = "AGE", ylab = "Birth weight")

points(birthwt[birthwt$smoke == 0,]$age, birthwt[birthwt$smoke == 0,]$bwt, type = "p", pch = 12, col = "green")

abline(fit_smoke, col = "red")
abline(fit_nonsmoke, col = "green")










## The following example show us that the importance and significance of one explanatory variable can be affected by presence
## or absence of other explanatory variables in the model. So our inference regarding the relationship between the response 
## variable and an explanatory variable could depend on what other variables we include in the model. Therefore, we should
## choose the set of explanatory variables very carefully when we perform statistical inference using multiple linear 
## regression models.


## install.packages("mfp", dependencies = TRUE)
library(mfp)
data(bodyfat)


fit <- lm(siri ~ height, data = bodyfat)

fit
##Call:
##lm(formula = siri ~ height, data = bodyfat)
##
##Coefficients:
##(Intercept)       height  
##    33.4945      -0.2045 

## pay attention here, we add only one predictor in this model, but this result turns out not to be significant
## the pvalue of height is only  0.15664 
summary(fit)
##Call:
##lm(formula = siri ~ height, data = bodyfat)
##
##Residuals:
##     Min       1Q   Median       3Q      Max 
##-19.5902  -6.7124   0.3966   6.0716  27.0919 
##
##Coefficients:
##            Estimate Std. Error t value Pr(>|t|)   
##(Intercept)  33.4945    10.1096   3.313  0.00106 **
##height       -0.2045     0.1439  -1.421  0.15664   
##---
##Signif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1
##
##Residual standard error: 8.352 on 250 degrees of freedom
##Multiple R-squared:  0.008009,	Adjusted R-squared:  0.004041 
##F-statistic: 2.019 on 1 and 250 DF,  p-value: 0.1566

names(fit)
##[1] "coefficients"  "residuals"     "effects"       "rank"          "fitted.values" "assign"        "qr"            "df.residual"   "xlevels"      
##[10] "call"          "terms"         "model" 

fit$coefficients
##(Intercept)      height 
## 33.4944938  -0.2044753

fit$fitted.values[1:5]
##       1        2        3        4        5 
##19.64129 18.72115 19.94800 18.72115 18.92563 

fit$residuals[1:5]
##         1          2          3          4          5 
## -7.341291 -12.621152   5.351996  -8.321152   9.774373 

plot(bodyfat$height, bodyfat$siri, main = "Scatterplot for Percent Body Fat by Height", xlab = "Height", ylab = "Percent Body Fat")
abline(fit)








## pay attention here, we add another predictor in this model, but this time both predictors turns out to be significant
## the pvalue of height and abdomen is 7.95e-06 and < 2e-16 , both are very significant
multReg <- lm(siri ~ height + abdomen, data = bodyfat)

summary(multReg)
##Call:
##lm(formula = siri ~ height + abdomen, data = bodyfat)
##
##Residuals:
##     Min       1Q   Median       3Q      Max 
##-18.8513  -3.4825  -0.0156   3.0949  11.1633 
##
##Coefficients:
##             Estimate Std. Error t value Pr(>|t|)    
##(Intercept) -14.31075    6.04265  -2.368   0.0186 *  
##height       -0.37053    0.08122  -4.562 7.95e-06 ***
##abdomen       0.64236    0.02759  23.283  < 2e-16 ***
##---
##Signif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1
##
##Residual standard error: 4.695 on 249 degrees of freedom
##Multiple R-squared:  0.6878,	Adjusted R-squared:  0.6853 
##F-statistic: 274.2 on 2 and 249 DF,  p-value: < 2.2e-16





















