##############################################
## test of getting cookie list
##############################################


myheader <- c(
  "User-Agent"="Mozilla/5.0 (Windows NT 6.1; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/45.0.2454.101 Safari/537.36",
  "Accept"= "*/*",
  "Accept-Language"= "zh-CN,zh;q=0.8",
  "Connection"="keep-alive",
  "Accept-Charset"="GB2312,utf-8;q=0.7,*;q=0.7"
)


curl <- getCurlHandle()

txt <- getURL("www.baidu.com", curl = curl, httpheader = myheader, verbose = T)




##############################################
## test of scrape elemets from a website
##############################################

myheader <- c(
  "User-Agent"="Mozilla/5.0 (Windows NT 6.1; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/45.0.2454.101 Safari/537.36",
  "Accept"= "*/*",
  "Accept-Language"= "zh-CN,zh;q=0.8",
  "Connection"="keep-alive",
  "Accept-Charset"="GB2312,utf-8;q=0.7,*;q=0.7"
)


d <- debugGatherer()
tmp <- getURL(url = "http://cos.name/", httpheader = myheader, debugfunction = d$update, verbose = T)
write.table(tmp, file = "test.html", row.names = F, col.names = F)
## this will return the result like we save the website as the html files


tmp <- getURL(url = "http://www.163.com/", httpheader = myheader, debugfunction = d$update, verbose = T)
write.table(tmp, file = "test.html", row.names = F, col.names = F)
## this will no return the result, it seems that the big website have some technologies to for anti-crawler


url <- "http://genome.ucsc.edu/cgi-bin/hgTracks?db=hg19&lastVirtModeType=default&lastVirtModeExtraState=&virtModeType=default&virtMode=0&nonVirtPosition=&position=chr13%3A38136719-38172981&hgsid=591169359_LiBQ9aaRmUJp4To1Y1AdA9xxi91c"

tmp <- getURL(url = url, httpheader = myheader, debugfunction = d$update, verbose = T)
write.table(tmp, file = "ucsc_test.html", row.names = F, col.names = F)







## in UCSC, each time you land on the website, we will be alloted a hgsid:xxxxxxxxx, a 9 numbers long seq. Which turns out to be a very important key for our further searching in the UCSC website.
## The following experiment shows this mechanism. First is the address when we land on UCSC in firefox, second is the address when we land on UCSC in Chrome.
## We are browsing the same gene in UCSC, but the hgsid number is completely different.

## 1    http://genome.ucsc.edu/cgi-bin/hgTracks?db=hg38&lastVirtModeType=default&lastVirtModeExtraState=&virtModeType=default&virtMode=0&nonVirtPosition=&position=chr13%3A37562589-37598844&hgsid=591168395_FhaTNGIdDzdV3y0KYwEdlaBrr5bX


## 2    http://genome.ucsc.edu/cgi-bin/hgTracks?db=hg38&lastVirtModeType=default&lastVirtModeExtraState=&virtModeType=default&virtMode=0&nonVirtPosition=&position=chr13%3A37562589-37598844&hgsid=591167645_MkOCYV3smNjf21JApuJjupoYBQ2E

##############################################
## test of landing on website
##############################################

myheader <- c(
  "User-Agent"="Mozilla/5.0 (Windows NT 6.1; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/45.0.2454.101 Safari/537.36",
  "Accept"= "*/*",
  "Accept-Language"= "zh-CN,zh;q=0.8",
  "Connection"="keep-alive",
  "Accept-Charset"="GB2312,utf-8;q=0.7,*;q=0.7"
)


curl <- getCurlHandle()
d <- debugGatherer()

txt <- getURL("http://cos.name/cn/", curl = curl, httpheader = myheader, verbose = T)
write.table(txt, file = "getcurl.html")


tmp <- getURL("www.baidu.com", debugfunction = d$update, httpheader = myheader, verbose = T)
write.table(tmp, file = "gatherer.html")




####################################################################
## test of simulate landing on website with name and password
####################################################################









